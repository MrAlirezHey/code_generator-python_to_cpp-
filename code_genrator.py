# -*- coding: utf-8 -*-
"""CODE_GENRATOR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wd_26eLecOfpN9_JXSU7iQIqorWCzj53
"""

!pip install -q --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
!pip install -q requests bitsandbytes transformers accelerate openai

from transformers import AutoTokenizer , AutoModelForCausalLM , TextStreamer ,BitsAndBytesConfig
from IPython.display import display,update_display , Markdown
import torch

model_name='Qwen/CodeQwen1.5-7B-Chat'

system_message = "You are an assistant that reimplements Python code in high performance C++ for an M1 Mac. "
system_message += "Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. "
system_message += "The C++ response needs to produce an identical output in the fastest possible time. Keep implementations of random number generators identical so that results match exactly."

def user_prompt_for(python):
    user_prompt = "Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. "
    user_prompt += "Respond only with C++ code; do not explain your work other than a few comments. "
    user_prompt += "Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\n\n"
    user_prompt += python
    return user_prompt

def message_for(python):
  message=[
      {'role':'system','content':system_message},
      {'role':'user','content':user_prompt_for(python)}
  ]
  return message

def write_cpp(cpp):
  code = cpp.replace("```cpp","").replace("```","")
  with open('optimized.cpp','w') as f:
    f.write(code)

pi = """
import time

def calculate(iterations, param1, param2):
    result = 1.0
    for i in range(1, iterations+1):
        j = i * param1 - param2
        result -= (1/j)
        j = i * param1 + param2
        result += (1/j)
    return result

start_time = time.time()
result = calculate(100_000_000, 4, 1) * 4
end_time = time.time()

print(f"Result: {result:.12f}")
print(f"Execution Time: {(end_time - start_time):.6f} seconds")
"""

quat_config=BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type='nf4'
)

tokenizer=AutoTokenizer.from_pretrained(model_name)
model=AutoModelForCausalLM.from_pretrained(model_name,device_map='auto',quantization_config=quat_config)

message=message_for(pi)
text=tokenizer.apply_chat_template(message,add_generation_prompt=True,return_tensors='pt').to('cuda')
streamer=TextStreamer(tokenizer)
response=model.generate(text,max_new_tokens=2000,streamer=streamer)

result=tokenizer.decode(response[0])

result

import re

match = re.search(r"```cpp\n(.*?)\n```", result, re.DOTALL)

code=match.group(1).strip()

print(code)

import gc

del model , tokenizer

gc.collect()
torch.cuda.empty_cache()

def code_generator(python,model_name='Qwen/CodeQwen1.5-7B-Chat'):


  gc.collect()
  torch.cuda.empty_cache()
  quant_config=BitsAndBytesConfig(
      load_in_4bit=True,
      bnb_4bit_use_double_quant=True,
      bnb_4bit_compute_dtype=torch.bfloat16,
      bnb_4bit_quant_type='nf4'
  )
  tokenizer=AutoTokenizer.from_pretrained(model_name)
  model=AutoModelForCausalLM.from_pretrained(model_name,device_map='auto',quantization_config=quant_config)
  prompt=message_for(python)
  input=tokenizer.apply_chat_template(prompt,add_generation_prompt=True,return_tensors='pt').to('cuda')
  output=model.generate(input,max_new_tokens=2000)
  response=tokenizer.decode(output[0])
  match=re.search(r"```cpp\n(.*?)\n```", result, re.DOTALL)
  code=match.group(1).strip()
  write_cpp(code)
  del model , tokenizer
  return code



code_generator(pi)

